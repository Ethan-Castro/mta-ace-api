# -*- coding: utf-8 -*-
"""Mta Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11w3Kny_9dxThaWDBLH65pW0v2HXyASWJ
"""

# Commented out IPython magic to ensure Python compatibility.
# # ============================================
# # 0) Setup & Installs
# # ============================================
# %%capture
# !pip -q install sqlalchemy psycopg2-binary pandas numpy pyarrow polars \
#   geopandas shapely pyproj rtree folium contextily \
#   statsmodels linearmodels scikit-learn xgboost shap \
#   pmdarima hdbscan networkx lifelines orjson tqdm matplotlib==3.9.0

# ============================================
# 1) Imports, DB Connection (READ-ONLY), Helpers
# ============================================
import os, warnings, datetime as dt
import numpy as np, pandas as pd, geopandas as gpd
from shapely.geometry import Point
import matplotlib.pyplot as plt
from sqlalchemy import create_engine, text, inspect
warnings.filterwarnings("ignore")

plt.rcParams["figure.figsize"] = (10, 5)
plt.rcParams["axes.grid"] = True
plt.rcParams["axes.titlesize"] = 14
plt.rcParams["font.size"] = 12

# ---- Neon connection (READ-ONLY behavior in this notebook) ----
DATABASE_URL = "postgresql://neondb_owner:npg_14EJbjQuZYMP@ep-tiny-firefly-ad4fba68-pooler.c-2.us-east-1.aws.neon.tech/neondb?sslmode=require"
engine = create_engine(DATABASE_URL, pool_pre_ping=True)

def read_sql(q, params=None):
    with engine.begin() as conn:
        return pd.read_sql(text(q), conn, params=params or {})

def table_exists(name, schema="public"):
    insp = inspect(engine)
    return name in insp.get_table_names(schema=schema)

def safe_select(sql, params=None, required_tables=()):
    for t in required_tables:
        if not table_exists(t):
            print(f"⏭️  Skipping: required table '{t}' not found.")
            return pd.DataFrame()
    try:
        return read_sql(sql, params)
    except Exception as e:
        print("⏭️  Skipping due to query error:", e)
        return pd.DataFrame()

def to_gdf(df, lon="longitude", lat="latitude", crs="EPSG:4326"):
    if df is None or df.empty:
        return gpd.GeoDataFrame(df if df is not None else pd.DataFrame(), geometry=[], crs=crs)
    return gpd.GeoDataFrame(df.copy(), geometry=gpd.points_from_xy(df[lon], df[lat]), crs=crs)

def to_2263(gdf):
    if gdf is None or gdf.empty:
        return gdf
    return gdf.to_crs("EPSG:2263")  # NYC feet

print("Connected to Neon ✅ (read-only behavior)")
print("Tables (first 12):", inspect(engine).get_table_names()[:12])

# ============================================
# 2) Data Overview (date coverage)
# ============================================
ov1 = safe_select('SELECT MIN("timestamp") AS min_ts, MAX("timestamp") AS max_ts, COUNT(*) AS n FROM bus_segment_speeds_2023_2024', required_tables=("bus_segment_speeds_2023_2024",))
ov2 = safe_select('SELECT MIN("timestamp") AS min_ts, MAX("timestamp") AS max_ts, COUNT(*) AS n FROM bus_segment_speeds_2025', required_tables=("bus_segment_speeds_2025",))
ov3 = safe_select('SELECT MIN(first_occurrence) AS min_ts, MAX(first_occurrence) AS max_ts, COUNT(*) AS n FROM violations', required_tables=("violations",))

print("bus_segment_speeds_2023_2024")
display(ov1.head(1))
print("bus_segment_speeds_2025")
display(ov2.head(1))
print("violations")
display(ov3.head(1))

# ============================================
# 3) CUNY Exposure Score (CES)
# ============================================
# Speeds -> stop-hour aggregates (per table)
q2324 = """
SELECT route_id, direction, timepoint_stop_id, timepoint_stop_name,
       AVG(timepoint_stop_latitude)  AS stop_lat,
       AVG(timepoint_stop_longitude) AS stop_lon,
       hour_of_day, AVG(bus_trip_count) AS trips_per_hour
FROM bus_segment_speeds_2023_2024
GROUP BY route_id, direction, timepoint_stop_id, timepoint_stop_name, hour_of_day
"""
q2025 = """
SELECT route_id, direction, timepoint_stop_id, timepoint_stop_name,
       AVG(timepoint_stop_latitude)  AS stop_lat,
       AVG(timepoint_stop_longitude) AS stop_lon,
       hour_of_day, AVG(bus_trip_count) AS trips_per_hour
FROM bus_segment_speeds_2025
GROUP BY route_id, direction, timepoint_stop_id, timepoint_stop_name, hour_of_day
"""
s1 = safe_select(q2324, required_tables=("bus_segment_speeds_2023_2024",))
s2 = safe_select(q2025, required_tables=("bus_segment_speeds_2025",))
speeds = pd.concat([d for d in [s1, s2] if not d.empty], ignore_index=True)

camp = safe_select("""SELECT campus, latitude AS campus_lat, longitude AS campus_lon FROM cuny_campus_locations""",
                   required_tables=("cuny_campus_locations",))

if camp.empty or speeds.empty:
    print("Missing campuses or speeds — CES skipped.")
else:
    stops = (speeds.groupby(["route_id","direction","timepoint_stop_id","timepoint_stop_name"], as_index=False)
             .agg(stop_lat=("stop_lat","mean"), stop_lon=("stop_lon","mean"),
                  trips_per_hour=("trips_per_hour","mean")))

    camp_g  = gpd.GeoDataFrame(camp, geometry=gpd.points_from_xy(camp["campus_lon"], camp["campus_lat"]), crs="EPSG:4326").to_crs(2263)
    stops_g = gpd.GeoDataFrame(stops, geometry=gpd.points_from_xy(stops["stop_lon"], stops["stop_lat"]), crs="EPSG:4326").to_crs(2263)

    camp_g["geometry"] = camp_g.buffer(1968)  # ~600m
    join = gpd.sjoin(stops_g, camp_g[["campus","geometry"]], predicate="within", how="inner").drop(columns=["index_right"])

    join["peak_weight"] = 1.2
    join["score"] = join["trips_per_hour"] * join["peak_weight"]

    ces = (join.groupby(["campus","route_id"])["score"]
           .sum().reset_index().rename(columns={"score":"ces_score"}))
    ces_top = (ces.sort_values(["campus","ces_score"], ascending=[True,False])
                  .groupby("campus").head(12))

    print("Top routes by CUNY Exposure Score (sample):")
    display(ces_top.head(24))

    # Small charts for the first few campuses
    for campus_name in ces_top["campus"].unique()[:3]:
        tmp = ces_top[ces_top["campus"]==campus_name].head(12)
        ax = tmp.plot(x="route_id", y="ces_score", kind="bar", title=f"Top CUNY Routes — {campus_name}")
        ax.set_xlabel("Route"); ax.set_ylabel("CES"); plt.show()

# ============================================
# 4) Route Speed Trends (daily)
# ============================================
qday = lambda table: f"""
SELECT route_id, DATE_TRUNC('day', "timestamp")::date AS d,
       AVG(average_road_speed) AS avg_speed_mph
FROM {table}
GROUP BY route_id, d
"""
d2324 = safe_select(qday("bus_segment_speeds_2023_2024"), required_tables=("bus_segment_speeds_2023_2024",))
d2025 = safe_select(qday("bus_segment_speeds_2025"), required_tables=("bus_segment_speeds_2025",))
daily = pd.concat([d for d in [d2324, d2025] if not d.empty], ignore_index=True)

if daily.empty:
    print("No route daily data.")
else:
    top_routes = (daily.groupby("route_id")["d"].nunique().sort_values(ascending=False).head(3).index.tolist())
    for r in top_routes:
        ts = daily[daily["route_id"]==r].sort_values("d")
        ax = ts.plot(x="d", y="avg_speed_mph", kind="line", title=f"Avg Daily Speed — {r}")
        ax.set_xlabel("Date"); ax.set_ylabel("mph"); plt.show()

    # JSON payload for API/UI if needed
    route_series = {r: daily[daily["route_id"]==r].sort_values("d")[["d","avg_speed_mph"]]
                    .rename(columns={"d":"date"}).to_dict(orient="records")
                    for r in daily["route_id"].unique()}
    print("Example series for:", top_routes[0], route_series[top_routes[0]][:3])

# ============================================
# 5) Repeat-Offender Survival (vehicle_id)
# ============================================
from lifelines import KaplanMeierFitter, CoxPHFitter

v = safe_select("""SELECT vehicle_id, first_occurrence AS ts FROM violations WHERE vehicle_id IS NOT NULL""",
                required_tables=("violations",))

if v.empty:
    print("No violations with vehicle_id.")
else:
    v = v.sort_values(["vehicle_id","ts"])
    v["ts"] = pd.to_datetime(v["ts"])
    v["next_ts"] = v.groupby("vehicle_id")["ts"].shift(-1)
    v["delta_days"] = (v["next_ts"] - v["ts"]).dt.total_seconds()/86400.0
    sv = v.dropna(subset=["delta_days"]).copy()
    sv["event"] = 1  # observed repeat (censoring at last obs not modeled here)

    # KM curve
    km = KaplanMeierFitter()
    km.fit(sv["delta_days"], event_observed=sv["event"], label="All vehicles")
    km.plot_survival_function()
    plt.title("Time to Next Violation — Kaplan–Meier")
    plt.xlabel("Days"); plt.ylabel("Survival probability")
    plt.show()

    # Cox PH
    if len(sv) >= 200:
        cph = CoxPHFitter()
        cph.fit(sv[["delta_days","event"]], duration_col="delta_days", event_col="event")
        cph.print_summary()
    else:
        print("Not enough samples for a stable Cox model.")

# ============================================
# 6) Violation Hotspots (DBSCAN, last 180 days)
# ============================================
from sklearn.cluster import DBSCAN

vi = safe_select("""
SELECT first_occurrence AS ts,
       violation_latitude  AS latitude,
       violation_longitude AS longitude
FROM violations
WHERE violation_latitude IS NOT NULL
  AND violation_longitude IS NOT NULL
  AND first_occurrence >= now() - INTERVAL '180 days'
""", required_tables=("violations",))

if vi.empty:
    print("No geocoded violations in last 180 days.")
else:
    g = to_gdf(vi, lon="longitude", lat="latitude")
    g2263 = to_2263(g)
    coords = np.vstack([g2263.geometry.x.values, g2263.geometry.y.values]).T

    if len(coords) < 100:
        print("Not enough points to cluster.")
        hotspots = pd.DataFrame()
    else:
        cl = DBSCAN(eps=200, min_samples=20).fit(coords)  # ~200 ft neighborhood
        g2263["cluster"] = cl.labels_
        hot = g2263[g2263["cluster"]>=0].copy()
        hot_counts = hot.groupby("cluster").size().reset_index(name="count")
        hot = hot.merge(hot_counts, on="cluster")

        # quick plot (FIXED)
        ax = hot.plot(markersize=3, column="count", legend=True)
        ax.set_title("Violation Hotspots (DBSCAN)")
        plt.show()

        # cluster centroids (for UI)
        centroids = (hot.groupby("cluster")["geometry"]
                     .apply(lambda s: gpd.GeoSeries(s).unary_union.centroid)
                     .reset_index())
        centroids_g = gpd.GeoDataFrame(centroids, geometry="geometry", crs=hot.crs).to_crs(4326)
        centroids_g["lat"] = centroids_g.geometry.y
        centroids_g["lon"] = centroids_g.geometry.x
        hotspots = centroids_g.merge(hot_counts, on="cluster")[["cluster","lat","lon","count"]].sort_values("count", ascending=False)

        print("Hotspot centroids (top 10):")
        display(hotspots.head(10))

# ============================================
# 7) Violations Forecasts (ALL routes, SARIMAX, parallel)
# ============================================
import pandas as pd, numpy as np, statsmodels.api as sm
from joblib import Parallel, delayed

# ---- knobs you can tweak ----
MIN_DAYS      = 60      # skip series shorter than this
HORIZON_DAYS  = 28      # forecast horizon
PARALLEL_JOBS = 4       # parallel workers (raise if Colab gives you more CPU)
PLOT_PREVIEW  = 0       # how many routes to plot (0 = none)

# Pull daily aggregates in SQL (fast even with ~3M rows)
daily = safe_select("""
SELECT bus_route_id AS route_id,
       DATE_TRUNC('day', first_occurrence)::date AS d,
       COUNT(*)::int AS violations
FROM violations
GROUP BY route_id, d
""", required_tables=("violations",))

# Snapshot date (optional, handy for UI banners / exports)
snap_df = safe_select("SELECT MAX(first_occurrence) AS max_ts FROM violations",
                      required_tables=("violations",))
snapshot_as_of = snap_df.iloc[0]["max_ts"] if not snap_df.empty else None

if daily.empty:
    print("No violations series.")
else:
    # Per-route SARIMAX forecaster
    def forecast_route(route_id: str, grp: pd.DataFrame):
        ts = (grp.sort_values("d")
              .set_index("d")["violations"]
              .asfreq("D").fillna(0))
        if len(ts) < MIN_DAYS:
            return route_id, None, "short"
        try:
            mod = sm.tsa.SARIMAX(
                ts, order=(1,1,1), seasonal_order=(1,1,1,7),
                enforce_stationarity=False, enforce_invertibility=False
            )
            res = mod.fit(disp=False)
            fc  = res.get_forecast(steps=HORIZON_DAYS)
            out = {
                "history":  ts.tail(90).to_dict(),               # last 90 days
                "forecast": fc.predicted_mean.to_dict(),         # next HORIZON_DAYS
                "ci_low":   fc.conf_int().iloc[:,0].to_dict(),
                "ci_high":  fc.conf_int().iloc[:,1].to_dict(),
            }
            return route_id, out, None
        except Exception as e:
            return route_id, None, f"error: {e}"

    groups = {r: g for r, g in daily.groupby("route_id")}
    results = Parallel(n_jobs=PARALLEL_JOBS, backend="loky")(
        delayed(forecast_route)(r, groups[r]) for r in groups.keys()
    )

    # Collect outputs in the same variable name the rest of your notebook expects
    forecasts = {r: out for (r, out, err) in results if out is not None and err is None}
    skipped   = [r for (r, out, err) in results if err == "short"]
    errors    = {r: err for (r, out, err) in results if (err is not None and err != "short")}

    print(f"Snapshot as of: {snapshot_as_of}")
    print(f"Routes total: {len(groups):,}")
    print(f"Forecasts produced: {len(forecasts):,}")
    if skipped:
        print(f"Skipped (short series < {MIN_DAYS} days): {len(skipped):,}")
    if errors:
        print(f"Errors on {len(errors):,} routes (showing a few):")
        for i, (r, e) in enumerate(errors.items()):
            if i >= 5: break
            print("  ", r, "->", e)

    # Optional quick plot preview
    if PLOT_PREVIEW and forecasts:
        import matplotlib.pyplot as plt
        for i, (r, out) in enumerate(forecasts.items()):
            if i >= PLOT_PREVIEW: break
            hist = pd.Series(out["history"])
            pred = pd.Series(out["forecast"])
            ci_l = pd.Series(out["ci_low"])
            ci_h = pd.Series(out["ci_high"])
            ax = hist.plot(label="observed", title=f"Violations Forecast — {r}")
            pred.plot(ax=ax, label="forecast", alpha=.8)
            ax.fill_between(ci_l.index, ci_l.values, ci_h.values, alpha=.2)
            ax.legend(); plt.show()

# ============================================
# 8) Next-Placement Scoring (risk model)
# ============================================
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error

q_feats = lambda table: f"""
SELECT route_id, timepoint_stop_id, hour_of_day,
       AVG(average_road_speed) AS avg_speed_mph,
       AVG(bus_trip_count)     AS trips_per_hour
FROM {table}
GROUP BY route_id, timepoint_stop_id, hour_of_day
"""
f2324 = safe_select(q_feats("bus_segment_speeds_2023_2024"), required_tables=("bus_segment_speeds_2023_2024",))
f2025 = safe_select(q_feats("bus_segment_speeds_2025"), required_tables=("bus_segment_speeds_2025",))
feats = pd.concat([d for d in [f2324, f2025] if not d.empty], ignore_index=True)

viol_hour = safe_select("""
SELECT bus_route_id AS route_id,
       EXTRACT(HOUR FROM first_occurrence)::int AS hour_of_day,
       DATE_TRUNC('week', first_occurrence)::date AS week,
       COUNT(*) AS violations
FROM violations
GROUP BY route_id, hour_of_day, week
""", required_tables=("violations",))

if feats.empty or viol_hour.empty:
    print("Insufficient data for candidate ranking.")
else:
    # mean violations per route+hour
    target = (viol_hour.groupby(["route_id","hour_of_day"])["violations"]
              .mean().reset_index().rename(columns={"violations":"viol_mean"}))
    df = feats.merge(target, on=["route_id","hour_of_day"], how="left")
    df["viol_mean"] = df["viol_mean"].fillna(0)

    X = df[["avg_speed_mph","trips_per_hour"]].fillna(0)
    y = df["viol_mean"]

    if len(df) < 100:
        print("Not enough training rows.")
    else:
        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)
        model = XGBRegressor(n_estimators=600, max_depth=6, subsample=0.8, colsample_bytree=0.9,
                             learning_rate=0.05, random_state=42)
        model.fit(X_tr, y_tr)
        pred = model.predict(X_te)
        print("MAE:", mean_absolute_error(y_te, pred))

        df["risk_score"] = model.predict(X)
        df["rank_in_route"] = df.groupby("route_id")["risk_score"].rank(ascending=False, method="dense")
        top_candidates = (df.sort_values("risk_score", ascending=False)
                            .head(200)[["route_id","timepoint_stop_id","hour_of_day",
                                        "avg_speed_mph","trips_per_hour","risk_score","rank_in_route"]])
        print("Top candidates (sample):")
        display(top_candidates.head(20))

# ============================================
# 9) Daily Violations Anomaly Flags
# ============================================
daily = safe_select("""
SELECT bus_route_id AS route_id,
       DATE_TRUNC('day', first_occurrence)::date AS d,
       COUNT(*) AS violations
FROM violations
GROUP BY route_id, d
""", required_tables=("violations",))

if daily.empty:
    print("No violations series.")
else:
    flags = []
    for r, grp in daily.groupby("route_id"):
        s = grp.sort_values("d").set_index("d")["violations"].asfreq("D").fillna(0)
        mu = s.rolling(28, min_periods=14).mean()
        sd = s.rolling(28, min_periods=14).std().replace(0, np.nan)
        z = (s - mu) / sd
        f = z[np.abs(z)>=3.0].dropna()
        for d, zval in f.items():
            flags.append({"route_id": r, "date": d, "z": float(zval), "violations": int(s.loc[d])})
    anomalies = pd.DataFrame(flags).sort_values(["date","z"], ascending=[False, False])
    print("Recent anomalies:")
    display(anomalies.head(20))

# ============================================
# 10) Optional: Export artifacts (local only, no DB writes)
# ============================================
import json, os
out_dir = "/content/exports"
os.makedirs(out_dir, exist_ok=True)

# CES
try:
    ces_top.to_csv(os.path.join(out_dir, "ces_top.csv"), index=False)
    print("Exported CES:", os.path.join(out_dir, "ces_top.csv"))
except: pass

# Hotspots
try:
    hotspots.to_csv(os.path.join(out_dir, "hotspots_centroids.csv"), index=False)
    print("Exported Hotspots:", os.path.join(out_dir, "hotspots_centroids.csv"))
except: pass

# Speed trends
try:
    # Save per-route series minimally for one route example
    with open(os.path.join(out_dir, "route_series_example.json"), "w") as f:
        json.dump(route_series, f, default=str)
    print("Exported route series:", os.path.join(out_dir, "route_series_example.json"))
except: pass

# Forecasts
try:
    with open(os.path.join(out_dir, "violations_forecasts.json"), "w") as f:
        json.dump(forecasts, f, default=str)
    print("Exported forecasts:", os.path.join(out_dir, "violations_forecasts.json"))
except: pass

# Placement candidates
try:
    top_candidates.to_csv(os.path.join(out_dir, "placement_candidates.csv"), index=False)
    print("Exported placement candidates:", os.path.join(out_dir, "placement_candidates.csv"))
except: pass

# Anomalies
try:
    anomalies.to_csv(os.path.join(out_dir, "violations_anomalies.csv"), index=False)
    print("Exported anomalies:", os.path.join(out_dir, "violations_anomalies.csv"))
except: pass

"""# 1A. Save classical stats models & outputs"""

import joblib, json, os
os.makedirs("model_artifacts", exist_ok=True)

# --- Survival: you trained KM and (optionally) CoxPH ---
# (page 6–7) keep the fitted objects and a compact JSON for the site
survival_bundle = {}
if 'km' in globals():
    joblib.dump(km, "model_artifacts/km.pkl")
    # export survival function for quick plotting in UI
    sf = km.survival_function_.reset_index().rename(columns={km._label:'survival'})
    survival_bundle["km"] = sf.to_dict(orient="list")

if 'cph' in globals():
    joblib.dump(cph, "model_artifacts/cox.pkl")
    survival_bundle["cox_summary"] = cph.summary.reset_index().to_dict(orient="list")

with open("model_artifacts/survival.json","w") as f:
    json.dump(survival_bundle, f, default=str)

"""# 1B. Save hotspot clusters as GeoJSON (for fast map rendering)"""

import geopandas as gpd

# 'hotspots' created on pages 7–8 (centroids with lat/lon/count)
if 'hotspots' in globals() and not hotspots.empty:
    g = gpd.GeoDataFrame(hotspots.copy(),
                         geometry=gpd.points_from_xy(hotspots["lon"], hotspots["lat"]),
                         crs="EPSG:4326")
    g = g.sort_values("count", ascending=False)
    g.to_file("model_artifacts/hotspots.geojson", driver="GeoJSON")

"""# 1C. Save SARIMAX forecasts per-route"""

import json, os
import pandas as pd
import numpy as np

os.makedirs("model_artifacts", exist_ok=True)

def _to_iso_dates(index_like):
    """
    Return a list of ISO8601 strings for a datetime-like index/array.
    If tz-aware, convert to UTC then drop tz. If not datetime, string-cast.
    """
    try:
        idx = pd.DatetimeIndex(index_like)
    except Exception:
        return pd.Index(index_like).astype(str).tolist()
    if idx.tz is not None:
        # convert to UTC then drop tz info
        idx = idx.tz_convert("UTC").tz_localize(None)
    return idx.astype("datetime64[ms]").astype(str).tolist()

def _series_to_arrays(s: pd.Series, name="value"):
    s = s.sort_index()
    dates = _to_iso_dates(s.index)
    vals  = pd.to_numeric(s, errors="coerce").astype(float).tolist()
    return {"date": dates, name: vals}

def _make_json_safe(v):
    if isinstance(v, (np.generic,)):
        return v.item()
    if isinstance(v, (pd.Timestamp, pd.Timedelta)):
        return str(v)
    return v

# normalize forecasts: {route: {history, forecast, ci_low, ci_high, ...}}
if 'forecasts' in globals():
    norm = {}
    for route, payload in forecasts.items():
        outp = {}
        for k in ["history", "forecast", "ci_low", "ci_high"]:
            v = payload.get(k)
            if isinstance(v, pd.Series):
                label = {"history":"history", "forecast":"forecast", "ci_low":"ci_low", "ci_high":"ci_high"}[k]
                outp[k] = _series_to_arrays(v, name=label)
            elif isinstance(v, dict):
                # if the dict uses datetime-like keys, coerce via Series
                if v and not isinstance(next(iter(v.keys())), (str, int, float, bool, type(None))):
                    outp[k] = _series_to_arrays(pd.Series(v), name=k)
                else:
                    outp[k] = {str(kk): _make_json_safe(vv) for kk, vv in v.items()}
            else:
                outp[k] = _make_json_safe(v)
        # pass through any extra fields safely
        for k, v in payload.items():
            if k not in outp:
                outp[k] = _make_json_safe(v)
        norm[str(route)] = outp

    with open("model_artifacts/forecasts.json","w") as f:
        json.dump(norm, f)
    print("Saved model_artifacts/forecasts.json")

# keep a banner timestamp
if 'snapshot_as_of' in globals():
    with open("model_artifacts/snapshot.json","w") as f:
        json.dump({"snapshot_as_of": str(snapshot_as_of)}, f)
    print("Saved model_artifacts/snapshot.json")

"""# 1D. Save the XGBoost “next-placement” risk model + metadata"""

# page 10
feature_cols = ["avg_speed_mph","trips_per_hour"]
meta = {
    "feature_cols": feature_cols,
    "notes": "Target is mean weekly violations per route+hour; model ranks stop-hour risk."
}

if 'model' in globals():
    joblib.dump(model, "model_artifacts/xgb_risk.pkl")
    with open("model_artifacts/xgb_meta.json","w") as f:
        json.dump(meta, f)

# Optional: export precomputed top candidates for instant UI tables
if 'top_candidates' in globals() and not top_candidates.empty:
    top_candidates.to_json("model_artifacts/top_candidates.json", orient="records")

"""# Step 2 — Minimal FastAPI to serve what you exported

"""

# app.py
from fastapi import FastAPI, HTTPException, Query
from typing import Optional, List
import joblib, json, pandas as pd
import geopandas as gpd

app = FastAPI(title="MTA ACE Models")

# ----- Load artifacts on startup -----
forecasts = json.load(open("model_artifacts/forecasts.json"))  # route_id -> dict
snapshot  = json.load(open("model_artifacts/snapshot.json"))
xgb       = joblib.load("model_artifacts/xgb_risk.pkl")
xmeta     = json.load(open("model_artifacts/xgb_meta.json"))
hotspots_gj = json.load(open("model_artifacts/hotspots.geojson"))
survival = json.load(open("model_artifacts/survival.json"))

@app.get("/health")
def health(): return {"ok": True, **snapshot}

# ---------- Forecasts ----------
@app.get("/forecast/{route_id}")
def route_forecast(route_id: str):
    if route_id not in forecasts:
        raise HTTPException(404, f"No forecast for route {route_id}")
    return forecasts[route_id]

# ---------- Risk scoring ----------
@app.get("/risk/score")
def risk_score(avg_speed_mph: float, trips_per_hour: float):
    X = pd.DataFrame([[avg_speed_mph, trips_per_hour]], columns=xmeta["feature_cols"])
    score = float(xgb.predict(X)[0])
    return {"risk_score": score}

# Optional: rank top candidates (precomputed)
@app.get("/risk/top")
def risk_top(limit: int = 100):
    try:
        df = pd.read_json("model_artifacts/top_candidates.json")
    except Exception:
        raise HTTPException(404, "No precomputed candidates")
    return df.head(limit).to_dict(orient="records")

# ---------- Hotspots ----------
@app.get("/hotspots.geojson")
def hotspots_geojson():
    return hotspots_gj

# ---------- Survival outputs ----------
@app.get("/survival/km")
def survival_km():
    return survival.get("km", {})

# If you exported CoxPH summary:
@app.get("/survival/cox_summary")
def survival_cox_summary():
    return survival.get("cox_summary", {})